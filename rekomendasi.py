# -*- coding: utf-8 -*-
"""rekomendasi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z65Wgh4dzD59lLsHQz3kiBTGimwuvZ8E

# **Proyek Akhir: Membuat Model Sistem Rekomendasi**

Oleh Ridopandi Sinaga
---

# 1. **Library Import**
"""

import os, zipfile
import numpy as np
import pandas as pd
import tensorflow as tf

from google.colab import files
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import RootMeanSquaredError
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import seaborn as sns
import matplotlib.pyplot as plt
import random

"""# 2. Data Loading

## Environment and Kaggle Credential
"""

# install kaggle
!pip install -q kaggle

# Username dan key Kaggle API
uploaded = files.upload()

"""## Dataset download

"""

#kaggle permission
!chmod 600 /content/kaggle.json

! KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d arashnic/book-recommendation-dataset

#unzip dataset
local_zip = '/content/book-recommendation-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

"""# 3. Dataset Understanding"""

# membaca dataset
books_dataframe = pd.read_csv('Books.csv')
ratings_dataframe = pd.read_csv('Ratings.csv')
users_dataframe = pd.read_csv('Users.csv')

"""### Dataset Books"""

books_dataframe.head()

print('Total ISBN:', len(books_dataframe.ISBN.unique()))
print('Total judul:', len(books_dataframe['Book-Title'].unique()))
print('Total pengarang:', len(books_dataframe['Book-Author'].unique()))
print('Total penerbit:', len(books_dataframe['Publisher'].unique()))

books_dataframe.info()

books_dataframe.describe()

"""### Dataset Rating"""

ratings_dataframe

print('Total pengguna:', len(ratings_dataframe['User-ID'].unique()))
print('Total buku:', len(ratings_dataframe['ISBN'].unique()))
print('Total rating yang diterima:', len(ratings_dataframe))

ratings_dataframe.info()

ratings_dataframe.describe()

# plot
with sns.axes_style('whitegrid'):
    g = sns.catplot(x="Book-Rating", data=ratings_dataframe, aspect=2.0, kind='count')
    g.set_ylabels("Jumlah total peringkat")
    plt.title("Plot Jumlah Rating Buku")

# Menampilkan tgrafik
plt.show()

"""Rating 0 adalah rating buku terbanyak, yaitu sebanyak 700.000 an. Artinya ada banyak buku yang belum dikasih rating, akibatnya bias bisa terjadi dan akan mempengaruhi hasil analisis nantinya. Salah satu solusinya data rating 0 tersebut dapat dihapus pada tahap *data preparation* nanti.

### Dataset User
"""

users_dataframe

"""Pada atribut Age terdapat nilai NaN (Not a Number), perlu dilakukan pemrosesan lebih lanjutan pada tahap *data preparation* nanti."""

users_dataframe.info()

"""Ternyata NaN pada atribut Age dikarenakan ada missing value"""

age_missingvalue = users_dataframe['Age'].isnull().sum()
print('Pada atribut Age terdapat missing value sebanyak', age_missingvalue )

users_dataframe.describe()

"""# 4. Data Preprocessing

Tahap pra-pemrosesan data atau *data preprocessing* merupakan tahap yang perlu diterapkan sebelum melakukan proses pemodelan. Tahap ini adalah teknik yang digunakan untuk mengubah data mentah *(raw data*) menjadi data yang bersih (*clean data*) yang siap untuk digunakan pada proses selanjutnya. Dalam kasus ini, tahap *data preprocessing* dilakukan dengan menyesuaikan nama kolom atau atribut masing-masing *dataframe*, melakukan penggabungkan data ISBN, dan data *User* untuk melihat jumlah data secara keseluruhan.
"""

# memperbaiki nama atribut books_dataframe
books_dataframe.rename(columns={
    'ISBN'                : 'isbn',
    'Book-Title'          : 'book_title',
    'Book-Author'         : 'book_author',
    'Year-Of-Publication' : 'pub_year',
    'Publisher'           : 'publisher',
    'Image-URL-S'         : 'image_s_url',
    'Image-URL-M'         : 'image_m_url',
    'Image-URL-L'         : 'image_l_url'
}, inplace=True)

books_dataframe.head(3)

# memperbaiki nama atribut ratings_dataframe
ratings_dataframe.rename(columns={
    'User-ID'     : 'user_id',
    'ISBN'        : 'isbn',
    'Book-Rating' : 'book_rating'
}, inplace=True)

ratings_dataframe.head(3)

# memperbaiki nama atribut users_dataframe
users_dataframe.rename(columns={
    'User-ID'  : 'user_id',
    'Location' : 'location',
    'Age'      : 'age'
}, inplace=True)

users_dataframe.head(3)

# menggabungkan data ISBN
ISBNAll = np.concatenate((
    books_dataframe.isbn.unique(),
    ratings_dataframe.isbn.unique()
))

ISBNAll = np.sort(np.unique(ISBNAll))

print(f'Jumlah Buku berdasarkan ISBN : {len(ISBNAll)}')

# menggabungkan data user
UserAll = np.concatenate((
    ratings_dataframe.user_id.unique(),
    users_dataframe.user_id.unique()
))

UserAll = np.sort(np.unique(UserAll))

print(f'Jumlah Buku berdasarkan ISBN : {len(UserAll)}')

"""# 5. Data Preparation

### Missmatch Value Handling
"""

# cek in books
books_dataframe.isnull().sum()

"""Terdapat missing value pada atribut `book_author`, `publisher`, dan `image_l_url` dikarenakan jumlah data yang missing sedikit jika dibandingkan dengan total keseluruhan data, tidak masalah jika kita drop saja bagian data yang kosong atau *null* tersebut."""

# drop data books yang kosong
books = books_dataframe.dropna()

# cek kembali
books.isnull().sum()

# cek in ratings
ratings_dataframe.isnull().sum()

"""Pada dataframe `ratings` tidak ada nilai kosong atau *null* di setiap atributnya.

Pada data understanding telah disinggung melalui grafik histogram **Jumlah Rating Buku**, sebagian besar data rating buku adalah rating 0. Jika dibiarkan bisa mengakibatkan *bias data* pada saat membuat rekomendasi nantinya. Harusnya rating yang dipakai rating 1 sampai rating 10 saja. Sehingga salah satu solusinya data rating 0 tersebut dihapus. Atau bisa cukup di filter saja lalu di *assign* ke variabel lain.
"""

# ambil rating 1 sampai 10 saja
ratings = ratings_dataframe[ratings_dataframe.book_rating>0]

# plot
with sns.axes_style('whitegrid'):
    g = sns.catplot(x="book_rating", data=ratings, aspect=1.5, kind='count')
    g.set_ylabels("Jumlah total peringkat")
    plt.title("Plot Jumlah Rating Buku")

# Menampilkan tgrafik
plt.show()

"""Rating 1-10 berhasil di filter.

### Missing value Handling
"""

# cek in users
users_dataframe.isnull().sum()

users_dataframe['age']

(110762/278858)*100

"""Sebelumnya juga telah disinggung tampak seperti diatas terdapat nilai kosong pada *atribut* `age` sebanyak 110.762. Data yang kosong tersebut hampir 40% nya"""

fig = plt.figure()
ax = sns.boxplot(data =  users_dataframe['age'])
plt.show()

fig = plt.figure()
ax = sns.histplot(data = users_dataframe['age'], binwidth = 2)
plt.show()

"""

Juga tampak distribusi data miring atau mengandung outliers (banyak orang muda dan beberapa orang tua), sehingga lebih baik data kosong diganti dengan meannya karena selain mencegah kehilangan banyak informasi, merupakan variabel numerik, juga mean lebih dapat mentoleran data miring. Walaupun sebenarnya nantinya hal ini bisa mengakibatkan bias data, but i think it's good idea for now."""

# isi NaN pada age dengan meannya
users = users_dataframe.copy()
users['age'] = users['age'].fillna(users['age'].mean())

users.isnull().sum()

"""Nah, nilai `age` yang kosong telah berhasil diganti dengan `median`.

### Data Duplicate Handling
"""

# cek books
books.duplicated().sum()

# cek ratings
ratings.duplicated().sum()

# cek users
users.duplicated().sum()

"""Berdasarkan hasil kode diatas, tidak ditemukan adanya data duplikat.

Oke sekarang sepertinya data sudah aman, dan saatnya kita gabungkan data books dan ratings
"""

# gabungkan books dan ratings on key isbn
books_ratings = pd.merge(ratings, books, on=['isbn'])
books_ratings.head()

"""# 6. Modelling

Sebelum memulai *modelling*, jika kita perhatikan pada saat *data preprocessing* jumlah data terbilang cukup banyak sampai ratus ribuan dan jika ditotal bisa sampai jutaan. Hal ini tentu bisa memperlambat proses pembuatan model dikarenakan perlu waktu maupun biaya yang lebih untuk menyediakan *RAM*  atau *GPU* yang lebih besar. Oleh karena itu, data yang dipakai hanya 10.000 baris data buku dan 10.000 baris data rating.
"""

# ambil 10000 books pertama dan 5000 ratings pertama
books   = books[:10000]
ratings = ratings[:10000]

"""## Content-based Recommendation

Merekomendasikan item yang mirip dengan item yang disukai pengguna di masa lalu. *Content-based filtering* akan mempelajari profil minat pengguna baru berdasarkan data dari objek yang telah dinilai pengguna.

#### TF-IDF Vectorizer
"""

# Inisialisasi TfidfVectorizer
tfv = TfidfVectorizer()
# Melakukan perhitungan idf pada data cuisine
tfv.fit(books['book_author'])

"""Selanjutnya, melakukan fit dan transformasi atribut book yaitu `book_author` ke dalam bentuk matriks."""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfv.fit_transform(books['book_author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Matriks yang kita miliki berukuran (10000, 5575). Nilai 10.000 merupakan ukuran data buku dan 5575 merupakan matrik data *author*.

Untuk menghasilkan vektor tf-idf dalam bentuk matriks, kita menggunakan fungsi  [`todense()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.todense.html)
"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Selanjutnya, mari kita lihat matriks tf-idf nya"""

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan author
# Baris diisi dengan nama buku

pd.DataFrame(
    tfidf_matrix.todense(),
    columns = tfv.get_feature_names_out(),
    index   = books.book_title
).sample(20, axis=1).sample(10, axis=0)

"""#### Cosine Similiarity

Selanjutnya, kita akan menghitung derajat kesamaan (*similarity degree*) antar judul buku dengan teknik *cosine similarity* menggunakan fungsi [`cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html 'cosine_similarity - scikit-learn Documentation') dari library `sklearn`.
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Selanjutnya, mari kita lihat array *cosine similiarity* nya"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama buku
cosine_sim_df = pd.DataFrame(
    cosine_sim,
    columns=books['book_title'],
    index=books['book_title']
)

print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap nama buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""#### Recommendation Testing"""

cosine_sim_df

"""Membuat fungsi `author_recommendations` untuk menampilkan data buku yang direkomendasikan oleh algoritma sistem yang telah dibuat, dengan parameter masukan berupa `book_title` yang sudah pernah dibaca oleh *user*."""

def author_recommendations(book_title, similarity_data=cosine_sim_df, items=books[['book_title', 'book_author']], k=10):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    # Drop nama buku agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

books['book_title']

"""Selanjutnya, mari kita terapkan fungsi di atas untuk menemukan rekomendasi buku yang mirip dengan buku  Proxies. Sebelumnya saya cari dulu nama buku yang mau ditest"""

readed_book_title ="Adventures of Huckleberry Finn"

books[books.book_title.eq(readed_book_title)]

author_recommendations(readed_book_title).drop_duplicates()

"""Tampak 7 buku (sesuai banyak k) yang direkomendasikan bersesuaian dengan buku *Adventures of Huckleberry Finn*.

Sehingga , akurasi nya 7/10 * 100 = 70%

## Collaborative Filtering Recommendation

We do not want to find a similarity between users or books. we want to do that If there is user A who has read and liked x and y books, And user B has also liked this two books and now user A has read and liked some z book which is not read by B so we have to recommend z book to user B. This is what collaborative filtering is.

Sistem rekomendasi penyaringan kolaboratif (*Collaborative Filtering Recommendation*) adalah sistem rekomendasi yang merekomendasikan item yang mirip dengan preferensi pengguna di masa lalu, misalnya berdasarkan *rating* yang telah diberikan oleh pengguna di masa lalu.

#### Data Preparation

##### Label Encoding
"""

ratings.info()

"""Melakukan *encoding* pada fitur `user_id` dan `isbn` buku kedalam *indeks integer*."""

user_id_enc = ratings.user_id.unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_id_enc)}
user_encoded_to_user = {i: x for i, x in enumerate(user_id_enc)}

book_id_enc = ratings['isbn'].unique().tolist()
book_to_book_encoded = {x: i for i, x in enumerate(book_id_enc)}
book_encoded_to_book = {i: x for i, x in enumerate(book_id_enc)}

"""Mapping `user_id` dan `isbn` yang sudah di encode ke dalam *dataframe*."""

ratings

ratings['user'] = ratings.user_id.map(user_to_user_encoded)
ratings['book'] = ratings.isbn.map(book_to_book_encoded)

"""Cek beberapa hal dalam data seperti jumlah user, jumlah buku, dan rating minimal serta rating maksimal."""

ratings

num_users = len(user_encoded_to_user)
num_books = len(book_encoded_to_book)

min_ratings = min(ratings.book_rating)
max_ratings = max(ratings.book_rating)

ratings_dataframe['book_rating'] = ratings_dataframe['book_rating'].values.astype(np.float32)

print(num_users)
print(num_books)
print(f'Number of User: {num_users}, Number of Books: {num_books}, Min Rating: {min_ratings}, Max Rating: {max_ratings}')

"""#### Training Data and Validation Data Split

Melakukan pembagian data menjadi data training dan validasi. Namun sebelumnya, datanya diacak terlebih dahulu agar distribusinya menjadi random.
"""

# Mengacak dataset
ratings = ratings.sample(frac=1, random_state=412)
ratings

"""Melakukan pembagian *dataset* dengan rasio 80:20, yaitu 80% untuk data latih (*training data*) dan 20% untuk data uji (*validation data*).

Namun sebelumnya, kita perlu memetakan (mapping) data user dan book menjadi satu value terlebih dahulu. Lalu, buatlah book_rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training.
"""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = ratings[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = ratings['book_rating'].apply(lambda x: (x-min_ratings) / (max_ratings-min_ratings)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * ratings.shape[0])

xTrain, xVal, yTrain, yVal = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""#### Building and Training Process

Pembuatan model akan menggunakan kelas `RecommenderNet` dengan [*keras model class*](https://keras.io/api/models/model 'Model class - Keras Documentation').
"""

class RecommenderNet(tf.keras.Model):
    # Insialisasi fungsi
    def __init__(self, num_users, num_books, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_books = num_books
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(# layer embedding user
            num_users,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.user_bias      = layers.Embedding(num_users, 1)# layer embedding user bias
        self.book_embedding = layers.Embedding( # layer embeddings book
            num_books,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.book_bias = layers.Embedding(num_books, 1)#layer embedding book bias

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:,0])# memanggil layer embedding 1
        user_bias   = self.user_bias(inputs[:, 0])# memanggil layer embedding 2
        book_vector = self.book_embedding(inputs[:, 1])# memanggil layer embedding 3
        book_bias   = self.book_bias(inputs[:, 1])# memanggil layer embedding 4

        dot_user_book = tf.tensordot(user_vector, book_vector, 2)

        x = dot_user_book + user_bias + book_bias

        return tf.nn.sigmoid(x)# activation sigmoid

"""Selanjutnya, lakukan proses compile terhadap model."""

model = RecommenderNet(num_users, num_books, 50)

model.compile(
    optimizer = Adam(learning_rate=0.001),
    loss      = BinaryCrossentropy(),
    metrics   = [RootMeanSquaredError()]
)

"""Model ini menggunakan` Binary Crossentropy` untuk menghitung *loss function*, `Adam (Adaptive Moment Estimation)` sebagai *optimizer*, dan `root mean squared error (RMSE)` sebagai *metrics evaluation*.

Langkah berikutnya, mulailah proses training. Namun sebelumnya saya buatkan fungsi *callback* untuk menghentikan proses training jika metrik evaluasi yaitu `root_mean_squared_error` sudah kurang dari 0.1.
"""

class RMSECallback(tf.keras.callbacks.Callback):
    def __init__(self, threshold_rmse):
        super().__init__()
        self.threshold_rmse = threshold_rmse

    def on_epoch_end(self, epoch, logs={}):
        current_rmse = logs.get('root_mean_squared_error')
        if current_rmse is not None and current_rmse < self.threshold_rmse:
            print(f"\nRMSE telah mencapai nilai yang cukup rendah (< {self.threshold_rmse})!")
            self.model.stop_training = True

# Contoh penggunaan callback
threshold_rmse = 0.1  # Ganti nilai ambang sesuai kebutuhan Anda
rmse_callback = RMSECallback(threshold_rmse)

"""Training model"""

history = model.fit(
    x               = xTrain,
    y               = yTrain,
    batch_size      = 20,
    epochs          = 70,
    callbacks=[rmse_callback],
    validation_data = (xVal, yVal),
)

rmse     = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

loss     = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize = (12, 4))
plt.subplot(1, 2, 1)
plt.plot(rmse,     label='RMSE')
plt.plot(val_rmse, label='Validation RMSE')
plt.title('Training and Validation Error')
plt.xlabel('Epoch')
plt.ylabel('Root Mean Squared Error')
plt.legend(loc='lower right')

plt.subplot(1, 2, 2)
plt.plot(loss,     label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.show()

"""#### Recommendation Testing"""

# Pendefenisian ulang dataset books dan ratings
datasetBook   = books
datasetRating = ratings

"""**Untuk** mendapatkan rekomendasi buku yang akan dihasilkan oleh sistem, diperlukan sebuah data atau sampel dari pengguna secara acak dan mendefinisikan variabel buku yang belum pernah dibaca oleh pengguna atau `notReadedBooks` yang merupakan daftar buku yang nantinya akan direkomendasikan. Daftar tersebut dapat didapatkan dengan menggunakan operator logika bitwise ([`~`](https://docs.python.org/3/reference/expressions.html#unary-arithmetic-and-bitwise-operations 'Unary Arithmetic and Bitwise Operations - Python Documentation')) pada variabel buku yang telah dibaca oleh pengguna atau`readedBooks`."""

userId = datasetRating.user_id.sample(1).iloc[0]
readedBooks = datasetRating[datasetRating.user_id == userId]

notReadedBooks = datasetBook[~datasetBook['isbn'].isin(readedBooks.isbn.values)]['isbn']
notReadedBooks = list(
    set(notReadedBooks).intersection(set(book_to_book_encoded.keys()))
)

notReadedBooks = [[book_to_book_encoded.get(x)] for x in notReadedBooks]
userEncoder    = user_to_user_encoded.get(userId)
userBookArray = np.hstack(
    ([[userEncoder]] * len(notReadedBooks), notReadedBooks)
)

"""Selanjutnya, untuk memperoleh rekomendasi buku, gunakan fungsi  [`model.predict()`](https://keras.io/api/models/model/) dari library Keras dengan menerapkan kode berikut."""

ratings = model.predict(userBookArray).flatten()

topRatingsIndices   = ratings.argsort()[-10:][::-1]
recommendedBookIds = [
    book_encoded_to_book.get(notReadedBooks[x][0]) for x in topRatingsIndices
]

print('Showing recommendations for users: {}'.format(userId))
print('=====' * 8)
print('Book with high ratings from user')
print('-----' * 8)

topBookUser = (
    readedBooks.sort_values(
        by = 'book_rating',
        ascending=False
    )
    .head(5)
    .isbn.values
)

bookDfRows = datasetBook[datasetBook['isbn'].isin(topBookUser)]
for row in bookDfRows.itertuples():
    print(row.book_title, ':', row.book_author)

print('=====' * 8)
print('Top 10 Books Recommendation')
print('-----' * 8)

recommended_resto = datasetBook[datasetBook['isbn'].isin(recommendedBookIds)]
for row in recommended_resto.itertuples():
    print(row.book_title, ':', row.book_author)

"""# 7. Kesimpulan"""